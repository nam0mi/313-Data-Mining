{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 313 - Educational data mining — Naomi, Sinchana, Parshana, Joanna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To run, go to run > run all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy matplotlib scikit-learn keras tensorflow nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error, root_mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1 - EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = pd.read_csv(\"S19_All_Release_2_10_22/Data/MainTable.csv\")\n",
    "df_subj = pd.read_csv(\"S19_All_Release_2_10_22/Data/LinkTables/Subject.csv\")\n",
    "df_problems = pd.read_csv(\"2nd CSEDM Data Challenge - Problem Prompts & Concepts Used - Sheet1.CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique students - using MainTable.csv\n",
    "len(df_main['SubjectID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique students - using LinkTables/Subject.csv\n",
    "len(df_subj['SubjectID'].unique())\n",
    "# Less because some people did not take the final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using MainTable.csv, there are 413 unique students. Using LinkTables/Subject.csv, which has students’ final scores, there are 372 students. We believe the number of students differ because some may not have taken the final. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_early = pd.read_csv(\"S19_All_Release_2_10_22/early.csv\")\n",
    "df_late = pd.read_csv(\"S19_All_Release_2_10_22/late.csv\")\n",
    "df_all = pd.concat([df_early, df_late])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_attempts = df_all.groupby(['AssignmentID', 'ProblemID'])['Attempts'].mean()\n",
    "mean_attempts.head()\n",
    "print(f\"Assignment {mean_attempts.idxmax()[0]} Problem {mean_attempts.idxmax()[1]} had the most attempts at {mean_attempts.max()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Assignment 487.0 Problem 102 had the most attempts at 10.422535211267606. We combined the early and late tables, then averaged the number of Attempts grouped by AssignmentID and ProblemID and got the maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = pd.read_csv(\"S19_All_Release_2_10_22/Data/MainTable.csv\")\n",
    "df_compile = df_main[df_main['EventType'] == \"Compile\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_errors = df_compile[df_compile['Compile.Result'] == 'Error']\n",
    "grouped_compile_errors = compile_errors.groupby(['AssignmentID', 'ProblemID'])['Compile.Result'].size()\n",
    "print(f\"Assignment {grouped_compile_errors.idxmax()[0]} Problem {grouped_compile_errors.idxmax()[1]} had the most compile errors at {grouped_compile_errors.max()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Computing the total number of compiler errors faced on each problem, and report the maximum is more meaningful because it directly answers which problems had the most compile errors from all students. This is more meaningful for knowing which problems students as a whole struggled with compiling. Assignment 439.0 Problem 13 had the most compile errors at 2440."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Open-Ended Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Analysis 1: Analyzing frequency of errors changing over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = df_main[df_main['EventType'] == \"Compile.Error\"][['ServerTimestamp','AssignmentID', 'ProblemID', 'CompileMessageData']]\n",
    "\n",
    "messages['ServerTimestamp'] = pd.to_datetime(messages['ServerTimestamp'])\n",
    "messages['Date'] = messages['ServerTimestamp'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['CompileMessageData'] = messages['CompileMessageData'].apply(\n",
    "    lambda x: re.sub(r\"line \\d+: (error: |\\[empty\\])?\", \"\", x)\n",
    ")\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CCategorize compile error messages into different types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_error(msg):\n",
    "    # Syntax Error\n",
    "    if re.search(r\"(illegal|cannot find symbol|character literal|illegal start of|illegal escape character|illegal character|unclosed string literal|illegal line end in character literal|invalid method declaration; return type required\\.|array dimension missing|bad operand type int for unary operator '!'|bad initializer for for-loop|array required, but int found|no suitable constructor found for String\\(char\\)|not a statement|missing return statement|integer number too large)\", msg, re.IGNORECASE):\n",
    "        return \"Syntax Error\"\n",
    "    # Type Error\n",
    "    elif re.search(r\"(bad operand type|dereferenced|type not allowed|unexpected type|char cannot be dereferenced|incompatible types|incomparable types|cannot be converted|bad operand types|int cannot be dereferenced|array dimension missing|incomparable types: boolean and int)\", msg, re.IGNORECASE):\n",
    "        return \"Type Error\"\n",
    "    # Variable/Method not found\n",
    "    elif re.search(r\"(variable .+ is already defined|cannot assign a value to final variable|cannot assign a value to final variable this|cannot find symbol: method|cannot find symbol: variable|cannot find symbol: class)\", msg, re.IGNORECASE):\n",
    "        return \"Variable/Method not found\"\n",
    "    # Initialization Issues\n",
    "    elif re.search(r\"(variable declaration not allowed here|variable .+ might not have been initialized|invalid method declaration|class expected|class .+ is already defined)\", msg, re.IGNORECASE):\n",
    "        return \"Initialization Issues\"\n",
    "    # Control Flow\n",
    "    elif re.search(r\"(for-each not applicable to expression type|orphaned case|break outside switch or loop|unreachable statement|'else' without 'if'|reached end of file|empty statement after if|'finally' without 'try'|return outside method|while expected)\", msg, re.IGNORECASE):\n",
    "        return \"Control Flow\"\n",
    "    # Class/Interface declaration\n",
    "    elif re.search(r\"(class, interface, or enum expected)\", msg, re.IGNORECASE):\n",
    "        return \"Class/Interface declaration\"\n",
    "    # Method Argument Errors\n",
    "    elif re.search(r\"(required, but|missing method|method .+ already defined|method .+ cannot be applied to given types|no arguments|no suitable method found)\", msg, re.IGNORECASE):\n",
    "        return(\"Method Argument\")\n",
    "    # Character expected\n",
    "    elif re.search(r\"(: expected|';' expected|'\\)' expected|'\\]' expected|'\\[' expected|'\\(' expected|; expected|> expected|' expected)\", msg, re.IGNORECASE):\n",
    "        return(\"Char Expected\")\n",
    "    else:\n",
    "        return \"Other\"\n",
    "messages['ErrorCategory'] = messages['CompileMessageData'].apply(categorize_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_counts = messages.groupby(['Date', 'ErrorCategory']).size().unstack(fill_value=0)\n",
    "\n",
    "# Plotting the line graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "for category in error_counts.columns:\n",
    "    plt.plot(error_counts.index, error_counts[category], label=category)\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Errors')\n",
    "plt.title('Error Categories Over Time')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "plt.legend(title='Error Category')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see spikes in errors presumably when an assignment is due, and students are compiling their code more often before the due date. The syntax errors and char expected errors are significantly higher than other types of errors. This is expected since novice programmers likely haven’t written enough code to check syntax thoroughly before compiling. Even advanced programmers forget a semicolon every now and then. Char expected errors also go hand in hand with syntax errors and can be linked to the same cause. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_messages = messages[~messages['ErrorCategory'].isin(['Syntax Error', 'Char Expected'])]\n",
    "error_counts_excluded = exclude_messages.groupby(['Date', 'ErrorCategory']).size().unstack(fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for category in error_counts_excluded.columns:\n",
    "    plt.plot(error_counts_excluded.index, error_counts_excluded[category], label=category)\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Errors')\n",
    "plt.title('Error Categories Over Time - Excluding Syntax Error and Char Expected')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "plt.legend(title='Error Category')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph is the same as the one above, but without syntax and char expected errors, which were an overwhelming majority, clouding our analysis. We now see a new trend in control flow and type errors. However, only type errors spike halfway through the term in this graph. This can be compared with lecture material and lab assignments to find out which assignment in particular cause confusion among students. We can hypothesize that there might have been new types introduced that the students struggled with. All of the other types of errors are relatively low but they do not necessarily decrease as the term goes on. In fact, they get higher midway through, then drop low again. This leads us to believe that there was either a very challenging assignment halfway through the quarter, or some new concept was introduced that really altered the students’ understanding of concepts they excelled at before. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Analysis 2: Predicting Score from problem features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We thought it would be interesting to see if students performance on specific question types would be a good predictor for their final exam grade. If so, this would be an indicator that the final exam was a good reflection of what the students had learned throughout the course. If not, it would show that there needs to be some adjustment in either the course material or the final exam to have better alignment with what was expected of the students to learn from the class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score = df_main[df_main['EventType'] == \"Run.Program\"]\n",
    "df_score = df_score.groupby(['SubjectID', 'AssignmentID', 'ProblemID'])['Score'].max().reset_index()\n",
    "score_problems = pd.merge(df_score, df_problems, on=['AssignmentID', 'ProblemID'], how='inner')\n",
    "score_problems.head()\n",
    "\n",
    "## collapse to one row per student by averaging the number of attempts per problem\n",
    "df_grades = pd.merge(df_score, df_all, on=['SubjectID', 'AssignmentID', 'ProblemID'], how='inner')\n",
    "df_grades = pd.merge(df_grades, df_subj, on=['SubjectID'], how='inner')\n",
    "df_grades = pd.merge(df_grades, df_problems, on=['AssignmentID', 'ProblemID'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_columns = ['SubjectID', 'AssignmentID', 'ProblemID', 'Attempts', 'Score', 'X-Grade','Requirement', 'If/Else',\n",
    "       'NestedIf', 'While', 'For', 'NestedFor', 'Math+-*/', 'Math%',\n",
    "       'LogicAndNotOr', 'LogicCompareNum', 'LogicBoolean', 'StringFormat',\n",
    "       'StringConcat', 'StringIndex', 'StringLen', 'StringEqual', 'CharEqual',\n",
    "       'ArrayIndex', 'DefFunction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_grades[important_columns]\n",
    "# grab the highest scoring attempt for each student for each problem\n",
    "df_max = df_model.loc[df_model.groupby(['SubjectID', 'AssignmentID', 'ProblemID'])['Score'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill the problem details with the score if thay concept was present in the problem.\n",
    "def replace_problem_details_with_score(df):\n",
    "    problem_detail_cols = ['If/Else', 'NestedIf', 'While', 'For',\n",
    "       'NestedFor', 'Math+-*/', 'Math%', 'LogicAndNotOr', 'LogicCompareNum',\n",
    "       'LogicBoolean', 'StringFormat', 'StringConcat', 'StringIndex',\n",
    "       'StringLen', 'StringEqual', 'CharEqual', 'ArrayIndex', 'DefFunction']\n",
    "\n",
    "    for col in problem_detail_cols:\n",
    "        df[col] = df.apply(lambda row: row['Score'] if row[col] == 1 else row[col], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_updated = replace_problem_details_with_score(df_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the average grade of the concept for each student\n",
    "df_model = df_updated.groupby(['SubjectID', 'X-Grade'])[['If/Else',\n",
    "       'NestedIf', 'While', 'For', 'NestedFor', 'Math+-*/', 'Math%',\n",
    "       'LogicAndNotOr', 'LogicCompareNum', 'LogicBoolean', 'StringFormat',\n",
    "       'StringConcat', 'StringIndex', 'StringLen', 'StringEqual', 'CharEqual',\n",
    "       'ArrayIndex', 'DefFunction']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start building the models\n",
    "\n",
    "df_model = df_model.fillna(df_model.median())  ## fill missing values with the median\n",
    "\n",
    "df_model.reset_index(inplace=True)\n",
    "\n",
    "y = df_model['X-Grade']\n",
    "\n",
    "X = df_model[['If/Else',\n",
    "       'NestedIf', 'While', 'For', 'NestedFor', 'Math+-*/', 'Math%',\n",
    "       'LogicAndNotOr', 'LogicCompareNum', 'LogicBoolean', 'StringFormat',\n",
    "       'StringConcat', 'StringIndex', 'StringLen', 'StringEqual', 'CharEqual',\n",
    "       'ArrayIndex', 'DefFunction']]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "m, b = np.polyfit(y_test, y_pred, 1)\n",
    "y_fit = m*y_test+b\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5, color='blue')\n",
    "plt.plot(y_test, y_fit, color='red')\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('Actual Grades')\n",
    "plt.ylabel('Predicted Grades')\n",
    "plt.title('Actual vs Predicted Grades - Linear Regression')\n",
    "plt.show()\n",
    "RMSE = root_mean_squared_error(y_test, y_pred)\n",
    "print(f'Root Mean Squared Error: {RMSE}')\n",
    "print(f'R2: {r2_score(y_test, y_pred)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The RMSE of the linear regression model is about 0.182, which means that the model's final grade prediction is off by 18.2% on average. This is definitely worse than we were hoping for, but we then thought that maybe a linear regression model wasn't the best predictor based on the high dimensionality of our data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
    "nn.fit(X_train, y_train)\n",
    "y_pred_nn = nn.predict(X_test)\n",
    "m, b = np.polyfit(y_test, y_pred_nn, 1)\n",
    "y_fit = m*y_test+b\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_nn)\n",
    "plt.plot(y_test, y_fit, color='red')\n",
    "plt.title('Actual vs Predicted Grades (MLP)')\n",
    "plt.xlabel('Actual Grades') \n",
    "plt.ylabel('Predicted Grades')\n",
    "RMSE = root_mean_squared_error(y_test, y_pred_nn)\n",
    "print(f'Root Mean Squared Error: {RMSE}')\n",
    "print(f'R2: {r2_score(y_test, y_pred_nn)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We then tried a multi-layer perceptron for our prediction, and our predictions were still off by 17.2% on average. This was still our best performing model, nevertheless.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['root_mean_squared_error'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=250, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "y_pred_nn = model.predict(X_test)\n",
    "m, b = np.polyfit(y_test, y_pred_nn, 1)\n",
    "y_fit = m*y_test+b\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_nn)\n",
    "plt.plot(y_test, y_fit, color='red')\n",
    "plt.title('Actual vs Predicted Grades (Neural Network)')\n",
    "plt.xlabel('Actual Grades')\n",
    "plt.ylabel('Predicted Grades')\n",
    "RMSE = root_mean_squared_error(y_test, y_pred_nn)\n",
    "print(f'Root Mean Squared Error: {RMSE}')\n",
    "print(f'R2: {r2_score(y_test, y_pred_nn)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One last hope was to build our own NN and train it for longer to see if our model just needed more time to learn the patterns in our dataset. Still, our model was off by 17.5% on average.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you look at the each resulting graph of actual grades vs predicted grades, you can see that most of the predicitons hover around the 65-70% mark. The median of the final exam grades is 69% and the average is 66%, so this made us think that our model just learned to predict around the mean/median, and therefore found no correlation between question type performance and final exam grade.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The fact that question type performance was not a good predictor for final exam grade suggests that the final exam was an inaccurate representation of what the students had learned throughout the course. This kind of information could be very useful to the instructor or institution to either tweak their curriculum or final exam content to better suit the students' performance and learning in the course.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train vs test RMSE\n",
    "plt.plot(history.history['root_mean_squared_error'][200:], label='train')\n",
    "plt.plot(history.history['val_root_mean_squared_error'][200:], label='validation')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Train vs Validation RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Side note**, take everything with a grain of salt because the model is both underfit and overfit, suggesting that the model has learned the patterns of the training set too well and has insufficient data points to train on. I tried a few different tactics to combat this but this was the best I landed on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Analysis 2 Part 3: Attempting TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another interest we had was to look into question wording had an association with students' performance on those specific questions. This could advise instructors to change the wording of their problems to better suit students' learning patterns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find average score and attempts per problem\n",
    "df_problem_avg = df_max.groupby(['AssignmentID','ProblemID', 'Requirement'])[['Score', 'Attempts']].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "## problem wording preprocessing\n",
    "lemma = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemma.lemmatize(w.lower()) for w in tokens if w not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "X_text = vectorizer.fit_transform(df_problem_avg['Requirement']).toarray()\n",
    "\n",
    "X_attempts = mms.fit_transform(df_problem_avg[['Attempts']]) ## scale average attempts to be on the same scale as the text features\n",
    "\n",
    "X = np.hstack((X_text, X_attempts))\n",
    "\n",
    "y = df_problem_avg['Score']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "m, b = np.polyfit(y_test, y_pred, 1)\n",
    "y_fit = m*y_test+b\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.plot(y_test, y_fit, color='red')\n",
    "plt.xlabel('Actual Scores')\n",
    "plt.ylabel('Predicted Scores')\n",
    "plt.title('Actual vs Predicted Score - Linear Regression')\n",
    "plt.show()\n",
    "RMSE = root_mean_squared_error(y_test, y_pred)\n",
    "print(f'Root Mean Squared Error: {RMSE}')\n",
    "print(f'R2: {r2_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I tried a few different types of models (KNN, MLP, NN, GB, and RF) and landed on linear regression being the best model to predict problem performance based on wording and average attempts taken. The RMSE of this mode was 0.02 which means the model's predictions were off by about 2% for the problem's average score. This seemed great to me at first, but then I looked at the distribution of the average scores and they were all between 89-98%. This suggests that problem wording wasn't a good predictor for score performance either.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the most negatively impacting words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "coefficients = model.coef_[:-1]  # exclude the attempts coef\n",
    "\n",
    "feature_importance = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "\n",
    "feature_importance = feature_importance.sort_values(by='Coefficient') # sort by most negatively impacting words\n",
    "\n",
    "top_negative_words = feature_importance.head(20)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(top_negative_words['Feature'], top_negative_words['Coefficient'], color='red')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Words')\n",
    "plt.title('Top Words Associated with Lower Scores')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_problem_avg[df_problem_avg['Requirement'].str.contains('word', case=False, na=False)]#['Requirement'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_problem_avg[df_problem_avg['Requirement'].str.contains('word', case=False, na=False)]['Requirement'].tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The word that was the most associated with worse performance was \"word\". The data I used to train the model is 50 points large, which is small and suggests that the model is overfit, so I would hope that increasing number of data points would also increase the model's performance. Since the model is overfit, it's likely the mode learned that this word is almost uniquely associated with the problem with the worst average score, therefore weighted it most negatively.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another side note, if you look at the wording of the question \"Write a function in Java that implements the following logic: Given a string str and a non-empty word, return a version of the original string where all chars have been replaced by pluses (+), except for appearances of the word which are preserved unchanged.\" If I were taking this course, I'd also be confused by this wording and perform worse.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As some next steps, I would want to look at phrases rather than individual words. You can tell more about what's causing trouble by looking at the phrases rather than the unique words used in the problem statement.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Analysis 3: Investigating questions that had the highest number of variance and theorizing why students might have a high level of variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_early = pd.read_csv(\"S19_All_Release_2_10_22/early.csv\")\n",
    "df_late = pd.read_csv(\"S19_All_Release_2_10_22/late.csv\")\n",
    "df_subject = pd.read_csv(\"S19_All_Release_2_10_22/Data/LinkTables/Subject.csv\")\n",
    "df_all = pd.concat([df_early, df_late])\n",
    "df = pd.merge(df_all, df_subject, left_on = 'SubjectID', right_on= 'SubjectID')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=\"AssignmentID\")\n",
    "attempt_variance = df.groupby(['AssignmentID', 'ProblemID'])['Attempts'].var().reset_index()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='ProblemID', y='Attempts', hue='AssignmentID', data=attempt_variance, palette='Set2', order=attempt_variance.sort_values(['AssignmentID', 'ProblemID'])['ProblemID'])\n",
    "plt.xlabel(\"Problem ID\")\n",
    "plt.ylabel(\"Variance in Attempts\")\n",
    "plt.title(\"Variance in Attempts Per Problem Across Assignments\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(title=\"Assignment ID\")\n",
    "plt.show()\n",
    "# Investigate problems 101, 102 as the varrying number of students from high to low amount of attempts to significantly bigger than others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows the variance in attempts for each question. Questions 101 and 102 had the highest variance by far, which means some students got it immediately, while other took several tries before they got the question correct. We can infer that these questions may have been about a concept that some students were not understanding clearly, or that there was an issue in the way the questions were worded. There was clearly a misunderstanding on the educator’s part either about the students’ understanding of the concept, or the clarity of the assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_101 = df[(df['ProblemID'] == 101) & (df['AssignmentID'] == 487)]\n",
    "sns.boxplot(x=df_101['Attempts'], showfliers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 101 Question: “We want to make a package of goal kilos of chocolate. We have small bars (1 kilo each) and big bars (5 kilos each). Return the number of small bars to use, assuming we always use big bars before small bars. Return -1 if it can't be done.”\n",
    "\n",
    "This is a box plot of the attempts on question 101. It shows that 50% of students got between 1-6 attempts, 50% of students got between 6-20 attempts. These are wide quartiles and prompt us to look into what caused so much confusion. After reading the question, we can see that it was difficult to understand what was being asked of the students. The first sentence doesn’t give enough context for students to know what they are looking for, or returning, in the code. This is the first point of confusion. The questions continues on to explain what types of bars there are and the rules on how many of each to use. Students likely made assumptions about how many big bars to use, and what the big and small bars must add up to. After this analysis, it is more than likely that the question itself was worded in a way that made it difficult for students to understand. We can infer that most students got it correct after understanding what to do, but that is the part that caused so much variance in number of attempts. In the future, the educator should either clarify the question or not use it anymore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_102 = df[(df['ProblemID'] == 102) & (df['AssignmentID'] == 487)]\n",
    "sns.boxplot(x=df_102['Attempts'], showfliers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 102 Question: Given a string, if the same 2-character substring appears at both its beginning and end, return the contents of the string without the 2-character substring at the beginning. For example, \"HelloHe\" yields \"lloHe\". Note that the 2-character substring at the beginning may overlap with the one at the end, so \"Hi\" yields \"\". If the two characters at the start and end of the string do not match each other, return the original string unchanged.\n",
    "\n",
    "Question 102 asks for string manipulation. 50% of students got between 1-7 attempts, 50% of students got between 8-38 attempts. While the question itself is relatively clear, it does seem like there would be some edge cases, one of which is clarified in the prompt. For example, case sensitivity is not clarified in the question, so some students may have changed all the letters to lowercase for matching purposes. Additionally, strings such at “aa” or “” will return empty strings which could also cause issues for novice programmers. Test cases could also include cases with strings of length 1, which would return the same string. However, if the program the student wrote does not check string length then it would throw an error. This question has some ambiguity on how certain cases are handled which we can assume is the cause for large variance in attempts. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
