{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naomi, Parshana, Sinchana, Joanna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy matplotlib scikit-learn keras tensorflow nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error, root_mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1 - EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = pd.read_csv(\"S19_All_Release_2_10_22/Data/MainTable.csv\")\n",
    "df_subj = pd.read_csv(\"S19_All_Release_2_10_22/Data/LinkTables/Subject.csv\")\n",
    "df_problems = pd.read_csv(\"2nd CSEDM Data Challenge - Problem Prompts & Concepts Used - Sheet1.CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique students - using MainTable.csv\n",
    "len(df_main['SubjectID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique students - using LinkTables/Subject.csv\n",
    "len(df_subj['SubjectID'].unique())\n",
    "# Less because some people did not take the final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_early = pd.read_csv(\"S19_All_Release_2_10_22/early.csv\")\n",
    "df_late = pd.read_csv(\"S19_All_Release_2_10_22/late.csv\")\n",
    "df_all = pd.concat([df_early, df_late])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_attempts = df_all.groupby(['AssignmentID', 'ProblemID'])['Attempts'].mean()\n",
    "mean_attempts.head()\n",
    "print(f\"Assignment {mean_attempts.idxmax()[0]} Problem {mean_attempts.idxmax()[1]} had the most attempts at {mean_attempts.max()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = pd.read_csv(\"S19_All_Release_2_10_22/Data/MainTable.csv\")\n",
    "df_compile = df_main[df_main['EventType'] == \"Compile\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_errors = df_compile[df_compile['Compile.Result'] == 'Error']\n",
    "grouped_compile_errors = compile_errors.groupby(['AssignmentID', 'ProblemID'])['Compile.Result'].size()\n",
    "print(f\"Assignment {grouped_compile_errors.idxmax()[0]} Problem {grouped_compile_errors.idxmax()[1]} had the most compile errors at {grouped_compile_errors.max()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the total number of compiler errors faced on each problem, and report the maximum is more meaningful because it directly answers which problems had the most compile errors from all students. This is more meaningful for knowing which problems students as a whole struggled with compiling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Open-Ended Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Analysis 1: Analyzing frequency of errors changing over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = df_main[df_main['EventType'] == \"Compile.Error\"][['ServerTimestamp','AssignmentID', 'ProblemID', 'CompileMessageData']]\n",
    "\n",
    "messages['ServerTimestamp'] = pd.to_datetime(messages['ServerTimestamp'])\n",
    "messages['Date'] = messages['ServerTimestamp'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['CompileMessageData'] = messages['CompileMessageData'].apply(\n",
    "    lambda x: re.sub(r\"line \\d+: (error: |\\[empty\\])?\", \"\", x)\n",
    ")\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CCategorize compile error messages into different types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_error(msg):\n",
    "    # Syntax Error\n",
    "    if re.search(r\"(illegal|cannot find symbol|character literal|illegal start of|illegal escape character|illegal character|unclosed string literal|illegal line end in character literal|invalid method declaration; return type required\\.|array dimension missing|bad operand type int for unary operator '!'|bad initializer for for-loop|array required, but int found|no suitable constructor found for String\\(char\\)|not a statement|missing return statement|integer number too large)\", msg, re.IGNORECASE):\n",
    "        return \"Syntax Error\"\n",
    "    # Type Error\n",
    "    elif re.search(r\"(bad operand type|dereferenced|type not allowed|unexpected type|char cannot be dereferenced|incompatible types|incomparable types|cannot be converted|bad operand types|int cannot be dereferenced|array dimension missing|incomparable types: boolean and int)\", msg, re.IGNORECASE):\n",
    "        return \"Type Error\"\n",
    "    # Variable/Method not found\n",
    "    elif re.search(r\"(variable .+ is already defined|cannot assign a value to final variable|cannot assign a value to final variable this|cannot find symbol: method|cannot find symbol: variable|cannot find symbol: class)\", msg, re.IGNORECASE):\n",
    "        return \"Variable/Method not found\"\n",
    "    # Initialization Issues\n",
    "    elif re.search(r\"(variable declaration not allowed here|variable .+ might not have been initialized|invalid method declaration|class expected|class .+ is already defined)\", msg, re.IGNORECASE):\n",
    "        return \"Initialization Issues\"\n",
    "    # Control Flow\n",
    "    elif re.search(r\"(for-each not applicable to expression type|orphaned case|break outside switch or loop|unreachable statement|'else' without 'if'|reached end of file|empty statement after if|'finally' without 'try'|return outside method|while expected)\", msg, re.IGNORECASE):\n",
    "        return \"Control Flow\"\n",
    "    # Class/Interface declaration\n",
    "    elif re.search(r\"(class, interface, or enum expected)\", msg, re.IGNORECASE):\n",
    "        return \"Class/Interface declaration\"\n",
    "    # Method Argument Errors\n",
    "    elif re.search(r\"(required, but|missing method|method .+ already defined|method .+ cannot be applied to given types|no arguments|no suitable method found)\", msg, re.IGNORECASE):\n",
    "        return(\"Method Argument\")\n",
    "    # Character expected\n",
    "    elif re.search(r\"(: expected|';' expected|'\\)' expected|'\\]' expected|'\\[' expected|'\\(' expected|; expected|> expected|' expected)\", msg, re.IGNORECASE):\n",
    "        return(\"Char Expected\")\n",
    "    else:\n",
    "        return \"Other\"\n",
    "messages['ErrorCategory'] = messages['CompileMessageData'].apply(categorize_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_counts = messages.groupby(['Date', 'ErrorCategory']).size().unstack(fill_value=0)\n",
    "\n",
    "# Plotting the line graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "for category in error_counts.columns:\n",
    "    plt.plot(error_counts.index, error_counts[category], label=category)\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Errors')\n",
    "plt.title('Error Categories Over Time')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "plt.legend(title='Error Category')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see spikes in errors presumably when an assignment is due, and students are compiling their code more often before the due date. The syntax errors and char expected errors are significantly higher than other types of errors. This is expected since novice programmers likely haven’t written enough code to check syntax thoroughly before compiling. Even advanced programmers forget a semicolon every now and then. Char expected errors also go hand in hand with syntax errors and can be linked to the same cause. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_messages = messages[~messages['ErrorCategory'].isin(['Syntax Error', 'Char Expected'])]\n",
    "error_counts_excluded = exclude_messages.groupby(['Date', 'ErrorCategory']).size().unstack(fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for category in error_counts_excluded.columns:\n",
    "    plt.plot(error_counts_excluded.index, error_counts_excluded[category], label=category)\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Errors')\n",
    "plt.title('Error Categories Over Time - Excluding Syntax Error and Char Expected')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "plt.legend(title='Error Category')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph is the same as the one above, but without syntax and char expected errors, which were an overwhelming majority, clouding our analysis. We now see a new trend in control flow and type errors. However, only type errors spike halfway through the term in this graph. This can be compared with lecture material and lab assignments to find out which assignment in particular cause confusion among students. We can hypothesize that there might have been new types introduced that the students struggled with. All of the other types of errors are relatively low but they do not necessarily decrease as the term goes on. In fact, they get higher midway through, then drop low again. This leads us to believe that there was either a very challenging assignment halfway through the quarter, or some new concept was introduced that really altered the students’ understanding of concepts they excelled at before. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Analysis 2: Predicting Score from problem features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score = df_main[df_main['EventType'] == \"Run.Program\"]\n",
    "df_score = df_score.groupby(['SubjectID', 'AssignmentID', 'ProblemID'])['Score'].max().reset_index()\n",
    "df_score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_problems = pd.merge(df_score, df_problems, on=['AssignmentID', 'ProblemID'], how='inner')\n",
    "score_problems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_problems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## collapse to one row per student by averaging the number of attempts per problem\n",
    "\n",
    "df_grades = pd.merge(df_score, df_all, on=['SubjectID', 'AssignmentID', 'ProblemID'], how='inner')\n",
    "#df_grades = df_grades.groupby(['SubjectID', 'AssignmentID', 'ProblemID'])[['Attempts', 'Score']].max()\n",
    "df_grades = pd.merge(df_grades, df_subj, on=['SubjectID'], how='inner')\n",
    "df_grades = pd.merge(df_grades, df_problems, on=['AssignmentID', 'ProblemID'], how='inner')\n",
    "df_grades.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_columns = ['SubjectID', 'AssignmentID', 'ProblemID', 'Attempts', 'Score', 'X-Grade','Requirement', 'If/Else',\n",
    "       'NestedIf', 'While', 'For', 'NestedFor', 'Math+-*/', 'Math%',\n",
    "       'LogicAndNotOr', 'LogicCompareNum', 'LogicBoolean', 'StringFormat',\n",
    "       'StringConcat', 'StringIndex', 'StringLen', 'StringEqual', 'CharEqual',\n",
    "       'ArrayIndex', 'DefFunction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_grades[important_columns]\n",
    "df_model.head()\n",
    "df_max = df_model.loc[df_model.groupby(['SubjectID', 'AssignmentID', 'ProblemID'])['Score'].idxmax()]\n",
    "df_max.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_problem_details_with_score(df):\n",
    "    problem_detail_cols = ['If/Else', 'NestedIf', 'While', 'For',\n",
    "       'NestedFor', 'Math+-*/', 'Math%', 'LogicAndNotOr', 'LogicCompareNum',\n",
    "       'LogicBoolean', 'StringFormat', 'StringConcat', 'StringIndex',\n",
    "       'StringLen', 'StringEqual', 'CharEqual', 'ArrayIndex', 'DefFunction']\n",
    "\n",
    "    for col in problem_detail_cols:\n",
    "        df[col] = df.apply(lambda row: row['Score'] if row[col] == 1 else row[col], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_updated = replace_problem_details_with_score(df_max)\n",
    "\n",
    "df_updated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_updated.groupby(['SubjectID', 'X-Grade'])[['If/Else',\n",
    "       'NestedIf', 'While', 'For', 'NestedFor', 'Math+-*/', 'Math%',\n",
    "       'LogicAndNotOr', 'LogicCompareNum', 'LogicBoolean', 'StringFormat',\n",
    "       'StringConcat', 'StringIndex', 'StringLen', 'StringEqual', 'CharEqual',\n",
    "       'ArrayIndex', 'DefFunction']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Analysis 2 Part 2: Start building the model (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_model.fillna(df_model.median())  # or df.fillna(df.median())\n",
    "\n",
    "df_model.reset_index(inplace=True)\n",
    "\n",
    "y = df_model['X-Grade']\n",
    "\n",
    "X = df_model[['If/Else',\n",
    "       'NestedIf', 'While', 'For', 'NestedFor', 'Math+-*/', 'Math%',\n",
    "       'LogicAndNotOr', 'LogicCompareNum', 'LogicBoolean', 'StringFormat',\n",
    "       'StringConcat', 'StringIndex', 'StringLen', 'StringEqual', 'CharEqual',\n",
    "       'ArrayIndex', 'DefFunction']]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('Actual Grades')\n",
    "plt.ylabel('Predicted Grades')\n",
    "plt.title('Actual vs Predicted Grades - Linear Regression')\n",
    "#plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "plt.show()\n",
    "RMSE = root_mean_squared_error(y_test, y_pred)\n",
    "print(f'Root Mean Squared Error: {RMSE}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_knn)\n",
    "plt.xlabel('Actual Grades')\n",
    "plt.ylabel('Predicted Grades')\n",
    "plt.title('Actual vs Predicted Grades (KNN)')\n",
    "\n",
    "RMSE = root_mean_squared_error(y_test, y_pred_knn)\n",
    "print(f'Root Mean Squared Error: {RMSE}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_rf)\n",
    "plt.title('Actual vs Predicted Grades (Random Forest)')\n",
    "RMSE = root_mean_squared_error(y_test, y_pred_rf)\n",
    "print(f'Root Mean Squared Error: {RMSE}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_gb)\n",
    "plt.title('Actual vs Predicted Grades (Gradient Boosting)')\n",
    "RMSE = root_mean_squared_error(y_test, y_pred_gb)\n",
    "print(f'Root Mean Squared Error: {RMSE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred_gb, y_test - y_pred_gb)\n",
    "plt.xlabel('Predicted Grades')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted Grades (Gradient Boosting)')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
    "nn.fit(X_train, y_train)\n",
    "y_pred_nn = nn.predict(X_test)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_nn)\n",
    "plt.title('Actual vs Predicted Grades (Neural Network)')\n",
    "RMSE = root_mean_squared_error(y_test, y_pred_nn)\n",
    "print(f'Root Mean Squared Error: {RMSE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['root_mean_squared_error'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=250, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "y_pred_nn = model.predict(X_test)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_nn)\n",
    "plt.title('Actual vs Predicted Grades (Neural Network)')\n",
    "RMSE = root_mean_squared_error(y_test, y_pred_nn)\n",
    "print(f'Root Mean Squared Error: {RMSE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residuals\n",
    "y_pred_nn = y_pred_nn.flatten()\n",
    "plt.figure(figsize=(10, 6))\n",
    "residuals = y_test - y_pred_nn\n",
    "plt.scatter(y_pred_nn, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Grades')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted Grades')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train vs test RMSE\n",
    "plt.plot(history.history['root_mean_squared_error'][200:], label='train')\n",
    "plt.plot(history.history['val_root_mean_squared_error'][200:], label='validation')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Train vs Validation RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Analysis 2 Part 3: Attempting TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find average score and attempts per problem\n",
    "df_problem_avg = df_max.groupby(['AssignmentID','ProblemID', 'Requirement'])[['Score', 'Attempts']].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "## problem wording preprocessing\n",
    "lemma = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemma.lemmatize(w.lower()) for w in tokens if w not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = vectorizer.fit_transform(df_problem_avg['Requirement'])\n",
    "#df_problem_avg['tokenized'] = vectorizer.fit_transform(df_problem_avg['Requirement']).toarray().tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)\n",
    "mms = MinMaxScaler()\n",
    "X_text = vectorizer.fit_transform(df_problem_avg['Requirement']).toarray()  # Convert to NumPy array\n",
    "med = df_problem_avg['Score'].median()\n",
    "df_problem_avg['score_cat'] = df_problem_avg['Score'].apply(lambda x: 1 if x > med else 0)\n",
    "\n",
    "# Add additional numerical features\n",
    "#X_attempts = df_problem_avg[['Attempts']].values\n",
    "X_attempts = mms.fit_transform(df_problem_avg[['Attempts']])\n",
    "\n",
    "X = np.hstack((X_text, X_attempts))\n",
    "\n",
    "y_cat = df_problem_avg['score_cat']\n",
    "y = df_problem_avg['Score']\n",
    "#X = df_problem_avg[['tokenized', 'Attempts']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X, y_cat, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('Actual Grades')\n",
    "plt.ylabel('Predicted Grades')\n",
    "plt.title('Actual vs Predicted Score - Linear Regression')\n",
    "plt.show()\n",
    "RMSE = root_mean_squared_error(y_test, y_pred)\n",
    "print(f'Root Mean Squared Error: {RMSE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residuals\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred, y_test - y_pred)\n",
    "plt.xlabel('Predicted Grades')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted Grades')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get model coefficients\n",
    "coefficients = model.coef_[:-1]  # Exclude the last coefficient (Attempts)\n",
    "\n",
    "# Create a DataFrame for analysis\n",
    "feature_importance = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "\n",
    "# Sort by most negative coefficients (words that lead to worse scores)\n",
    "feature_importance = feature_importance.sort_values(by='Coefficient')\n",
    "\n",
    "# Display the top negative words\n",
    "top_negative_words = feature_importance.head(20)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(top_negative_words['Feature'], top_negative_words['Coefficient'], color='red')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Words')\n",
    "plt.title('Top Words Associated with Lower Scores')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_problem_avg[df_problem_avg['Requirement'].str.contains('word', case=False, na=False)]#['Requirement'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_problem_avg[df_problem_avg['Requirement'].str.contains('word', case=False, na=False)]['Requirement'].tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This problem has the worst performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_knn)\n",
    "plt.xlabel('Actual Grades')\n",
    "plt.ylabel('Predicted Grades')\n",
    "plt.title('Actual vs Predicted Grades (KNN)')\n",
    "RMSE = root_mean_squared_error(y_test, y_pred_knn)\n",
    "print(f'Root Mean Squared Error: {RMSE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Analysis 3: Investigating questions that had the highest number of variance and theorizing why students might have a high level of variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_early = pd.read_csv(\"S19_All_Release_2_10_22/early.csv\")\n",
    "df_late = pd.read_csv(\"S19_All_Release_2_10_22/late.csv\")\n",
    "df_subject = pd.read_csv(\"S19_All_Release_2_10_22/Data/LinkTables/Subject.csv\")\n",
    "df_all = pd.concat([df_early, df_late])\n",
    "df = pd.merge(df_all, df_subject, left_on = 'SubjectID', right_on= 'SubjectID')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=\"AssignmentID\")\n",
    "attempt_variance = df.groupby(['AssignmentID', 'ProblemID'])['Attempts'].var().reset_index()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='ProblemID', y='Attempts', hue='AssignmentID', data=attempt_variance, palette='Set2', order=attempt_variance.sort_values(['AssignmentID', 'ProblemID'])['ProblemID'])\n",
    "plt.xlabel(\"Problem ID\")\n",
    "plt.ylabel(\"Variance in Attempts\")\n",
    "plt.title(\"Variance in Attempts Per Problem Across Assignments\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(title=\"Assignment ID\")\n",
    "plt.show()\n",
    "# Investigate problems 101, 102 as the varrying number of students from high to low amount of attempts to significantly bigger than others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows the variance in attempts for each question. Questions 101 and 102 had the highest variance by far, which means some students got it immediately, while other took several tries before they got the question correct. We can infer that these questions may have been about a concept that some students were not understanding clearly, or that there was an issue in the way the questions were worded. There was clearly a misunderstanding on the educator’s part either about the students’ understanding of the concept, or the clarity of the assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_101 = df[(df['ProblemID'] == 101) & (df['AssignmentID'] == 487)]\n",
    "sns.boxplot(x=df_101['Attempts'], showfliers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 101 Question: “We want to make a package of goal kilos of chocolate. We have small bars (1 kilo each) and big bars (5 kilos each). Return the number of small bars to use, assuming we always use big bars before small bars. Return -1 if it can't be done.”\n",
    "\n",
    "This is a box plot of the attempts on question 101. It shows that 50% of students got between 1-6 attempts, 50% of students got between 6-20 attempts. These are wide quartiles and prompt us to look into what caused so much confusion. After reading the question, we can see that it was difficult to understand what was being asked of the students. The first sentence doesn’t give enough context for students to know what they are looking for, or returning, in the code. This is the first point of confusion. The questions continues on to explain what types of bars there are and the rules on how many of each to use. Students likely made assumptions about how many big bars to use, and what the big and small bars must add up to. After this analysis, it is more than likely that the question itself was worded in a way that made it difficult for students to understand. We can infer that most students got it correct after understanding what to do, but that is the part that caused so much variance in number of attempts. In the future, the educator should either clarify the question or not use it anymore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_102 = df[(df['ProblemID'] == 102) & (df['AssignmentID'] == 487)]\n",
    "sns.boxplot(x=df_102['Attempts'], showfliers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 102 Question: Given a string, if the same 2-character substring appears at both its beginning and end, return the contents of the string without the 2-character substring at the beginning. For example, \"HelloHe\" yields \"lloHe\". Note that the 2-character substring at the beginning may overlap with the one at the end, so \"Hi\" yields \"\". If the two characters at the start and end of the string do not match each other, return the original string unchanged.\n",
    "\n",
    "Question 102 asks for string manipulation. 50% of students got between 1-7 attempts, 50% of students got between 8-38 attempts. While the question itself is relatively clear, it does seem like there would be some edge cases, one of which is clarified in the prompt. For example, case sensitivity is not clarified in the question, so some students may have changed all the letters to lowercase for matching purposes. Additionally, strings such at “aa” or “” will return empty strings which could also cause issues for novice programmers. Test cases could also include cases with strings of length 1, which would return the same string. However, if the program the student wrote does not check string length then it would throw an error. This question has some ambiguity on how certain cases are handled which we can assume is the cause for large variance in attempts. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
